{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ahmed/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download([\n",
    "'punkt',\n",
    "'wordnet',\n",
    "'averaged_perceptron_tagger',\n",
    "'stopwords',\n",
    "'omw-1.4'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(list):#list of reviews\n",
    "    tokens = []\n",
    "    for review in list:\n",
    "        tokens.append(word_tokenize(review))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):# list of tokens\n",
    "    new_list = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            new_list.append(w)\n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens): #list of tokens\n",
    "    new_list = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for word in tokens:\n",
    "        new_list.append(stemmer.stem(word))\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_noise(tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() and token.isalpha():\n",
    "            cleaned_tokens.append(token.lower())\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from files\n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "for path in os.scandir('./data/neg'):\n",
    "    if path.is_file():\n",
    "        f = open(path,'r')\n",
    "        positive_reviews.append(f.read().lower())\n",
    "\n",
    "positive_list = list()\n",
    "for path in os.scandir('./data/pos'):\n",
    "    if path.is_file():\n",
    "        f = open(path,'r')\n",
    "        negative_reviews.append(f.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n",
      "716\n",
      "222\n",
      "578\n",
      "852\n"
     ]
    }
   ],
   "source": [
    "#pre processing\n",
    "#tokenization\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "positive_tokens = get_tokens(positive_reviews)\n",
    "negative_tokens = get_tokens(negative_reviews)\n",
    "\n",
    "print(len(positive_tokens[0]))\n",
    "print(len(negative_tokens[0]))\n",
    "\n",
    "positive_tokens = [remove_noise(tokens) for tokens in positive_tokens]\n",
    "negative_tokens = [remove_noise(tokens) for tokens in negative_tokens]\n",
    "\n",
    "print(len(positive_tokens[0]))\n",
    "print(len(negative_tokens[0]))\n",
    "\n",
    "\n",
    "positive_tokens = [remove_stop_words(tokens) for tokens in positive_tokens]\n",
    "negative_tokens = [remove_stop_words(tokens) for tokens in negative_tokens]\n",
    "\n",
    "\n",
    "positive_tokens = [join_tokens(tokens) for tokens in positive_tokens]\n",
    "\n",
    "negative_tokens = [join_tokens(tokens) for tokens in negative_tokens]\n",
    "\n",
    "print(len(positive_tokens[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walt disney studio may finally meet match lush animation twentieth century fox anastasia judge late effort bluth studio visuals thing fox brag disney recent classic occasionally stretch credibility film pocahontas hunchback notre dame less extent hercules anastasia fox go far throw fact completely window may say kid movie well young kid beware may noticeably frighten visuals rasputin zombie whose body part continually fall disconcertingly real way consider warn nevertheless animation quite stunning time bluth use computer animation extensively throughout occasionally rival photographic quality yet scene material seem tv crowd lead wonder rush market combat disney plot anyone read history know concern attempt return anastasia royal family lose overthrow romanov anastasia much concern really happen plot go rent disney candleshoe see anastasia\n"
     ]
    }
   ],
   "source": [
    "print(positive_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "X['reviews'] = positive_tokens + negative_tokens\n",
    "\n",
    "zeros = np.zeros(len(positive_tokens),dtype=int) \n",
    "ones = np.ones(len(positive_tokens),dtype=int)\n",
    "\n",
    "Y = pd.DataFrame()\n",
    "Y['sentiment'] = np.append(zeros,ones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.8033333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/anaconda3/envs/GpEnv/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1))\n",
    "text_counts= cv.fit_transform(X['reviews'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, Y, test_size=0.3,shuffle=True)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7bdb860da4d2148fc4261a2c0098754a86515641924a72ee06df4b7a006b4bb"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('GpEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
